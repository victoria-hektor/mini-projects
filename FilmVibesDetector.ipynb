{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "WB3RUgHR6m-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l5h0z-Vz4lqi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Preprocess the IMDB Dataset\n",
        "TensorFlow provides easy access to the IMDB dataset, preloaded with tokenized texts. However, for an advanced approach, we consider controlling the vocabulary size and padding sequences for uniform input length."
      ],
      "metadata": {
        "id": "WM8JQ9Xl6qOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocabulary size\n",
        "vocab_size = 5000\n",
        "\n",
        "# Load IMDB dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Set the maximum number of words per review\n",
        "max_words = 500\n",
        "\n",
        "# Pad sequences for uniform input size\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDBYcPc36qZl",
        "outputId": "9f4811d8-ee71-42ac-9081-ee77835351b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the LSTM Model\n",
        "We'll design the neural network with an Embedding layer, an LSTM layer, and a dense output layer with a sigmoid activation function for binary classification."
      ],
      "metadata": {
        "id": "OJRASMHA6zYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "embedding_size = 32\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\n",
        "model.add(Dropout(0.2))  # Dropout layer after embedding\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  # Adding dropout to LSTM\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Compile Diff Model\n",
        "# For multi-class classification, change the final layer and loss function\n",
        "# Uncomment Below to change to this\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2_9VY0v60Ji",
        "outputId": "345ca17c-5c3e-4579-d596-552b004ea372"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               53200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213301 (833.21 KB)\n",
            "Trainable params: 213301 (833.21 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3b: Apply Learning Rate Schedules and Early Stopping\n",
        "Learning rate schedules and early stopping are strategies to optimize the training process. TensorFlow allows you to easily implement these."
      ],
      "metadata": {
        "id": "ld__znc2BOPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import math\n",
        "\n",
        "# Define a learning rate schedule function\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Instantiate callbacks\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Train the model with callbacks\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=num_epochs, callbacks=[lr_scheduler, early_stopping])"
      ],
      "metadata": {
        "id": "1GP-lePUBUZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train the Model\n",
        "We'll train the model with the training data and evaluate its initial performance on the test set."
      ],
      "metadata": {
        "id": "iV2Lw9HZ66Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4-D1TEQ667J",
        "outputId": "4455ce24-855a-4a47-8613-68dbe9437f13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 384s 976ms/step - loss: 0.4914 - accuracy: 0.7588 - val_loss: 0.3560 - val_accuracy: 0.8535\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 382s 978ms/step - loss: 0.3000 - accuracy: 0.8811 - val_loss: 0.4282 - val_accuracy: 0.8104\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 388s 993ms/step - loss: 0.2757 - accuracy: 0.8903 - val_loss: 0.3220 - val_accuracy: 0.8684\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ac1ca663550>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Evaluate the Model\n",
        "After training, we evaluate the model using the test set to obtain metrics like accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "XX9IE90H6-Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# For precision, recall, and F1-score, we need to make predictions and compare with true labels\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTHSKXvy7AJ-",
        "outputId": "336f8b7e-41be-4aff-9390-ee5ffb70dc5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 81s 104ms/step - loss: 0.3220 - accuracy: 0.8684\n",
            "Test Accuracy: 86.84%\n",
            "782/782 [==============================] - 79s 100ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.86     12500\n",
            "           1       0.84      0.91      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Advanced Model Evaluation: Confusion Matrix and ROC Curve\n",
        "To evaluate your model more comprehensively, utilise the confusion matrix and ROC curve analysis."
      ],
      "metadata": {
        "id": "RMkPdKxOAkQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QkWY9Ej-AnyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}